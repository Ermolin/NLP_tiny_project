{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP самост работа \"Simple text classification task.ipynb\"",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ermolin/NLP_tiny_project/blob/main/NLP_%D1%81%D0%B0%D0%BC%D0%BE%D1%81%D1%82_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0_%22Simple_text_classification_task_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1DNiHsB0bvg"
      },
      "source": [
        "# Классификация текста простыми методами"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpFTvZC_j0ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "758d78a2-d16d-4a59-eac6-80beefd7d641"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivAOt423fyiv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26b346b9-6343-40f6-f3df-f6f51b255ad2"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = [\"comp.sys.mac.hardware\", \"comp.windows.x\",  \"misc.forsale\", \"rec.autos\", \"rec.motorcycles\", \"rec.sport.baseball\", \"rec.sport.hockey\", \"sci.crypt\", \"sci.electronics\", \"sci.med\",\n",
        "              \"sci.space\", \"soc.religion.christian\", \"talk.politics.guns\", \"talk.politics.mideast\"]\n",
        "              \n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
        "\n",
        "X_train = newsgroups_train.data\n",
        "y_train = newsgroups_train.target\n",
        "\n",
        "X_test = newsgroups_test.data\n",
        "y_test = newsgroups_test.target"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0qDNIBUhfXb",
        "outputId": "46c6f764-9c0f-4df1-a66d-2b1dc950f418"
      },
      "source": [
        "len(X_train), len(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8227, 8227)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkENH8hHtb81",
        "outputId": "1621753e-f7fe-43f5-cf53-dcba2421fcdc"
      },
      "source": [
        "len(X_test), len(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5477, 5477)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rgob4qmZB59"
      },
      "source": [
        "## Предобработка данных\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJoJPqJqZCxk"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PFTYSiaZg_g",
        "outputId": "fbdadd3c-2710-49c0-ad71-54521d3a7cd6"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def preprocces(X):\n",
        "  X_proccess = []\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  for x in tqdm(X):\n",
        "    \n",
        "    x = x.lower()\n",
        "    x = nltk.word_tokenize(x)\n",
        "    x = [word for word in x if word.isalnum()]\n",
        "    x = [lemmatizer.lemmatize(w) for w in x]\n",
        "    x = [word for word in x if not word in stop_words]\n",
        "    X_proccess.append(' '.join(x))\n",
        "  return X_proccess\n",
        "\n",
        "\n",
        "X_train_proc = preprocces(X_train)\n",
        "X_test_proc = preprocces(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8227/8227 [00:37<00:00, 217.30it/s]\n",
            "100%|██████████| 5477/5477 [00:22<00:00, 242.27it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHJZSr9SZqf3",
        "outputId": "868ce13b-fc9e-44b7-f495-bb8a2da7e4f1"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train_proc)\n",
        "X_test_vec = vectorizer.transform(X_test_proc)\n",
        "print(X_train_vec.shape)\n",
        "print(X_test_vec.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8227, 62488)\n",
            "(5477, 62488)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeiWLVq7wvBk"
      },
      "source": [
        "обучение на LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlowBHFXZtae",
        "outputId": "8d85da16-901f-42d8-f195-94fc2ca5c762"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train_vec, y_train)\n",
        "y_pred = lr.predict(X_test_vec)\n",
        "\n",
        "print(classification_report(y_true=y_test, y_pred=y_pred, target_names=newsgroups_train.target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        precision    recall  f1-score   support\n",
            "\n",
            " comp.sys.mac.hardware       0.90      0.92      0.91       385\n",
            "        comp.windows.x       0.92      0.93      0.93       395\n",
            "          misc.forsale       0.79      0.89      0.84       390\n",
            "             rec.autos       0.92      0.88      0.90       396\n",
            "       rec.motorcycles       0.96      0.94      0.95       398\n",
            "    rec.sport.baseball       0.94      0.95      0.94       397\n",
            "      rec.sport.hockey       0.96      0.96      0.96       399\n",
            "             sci.crypt       0.97      0.91      0.94       396\n",
            "       sci.electronics       0.78      0.87      0.82       393\n",
            "               sci.med       0.94      0.87      0.91       396\n",
            "             sci.space       0.97      0.94      0.95       394\n",
            "soc.religion.christian       0.93      0.95      0.94       398\n",
            "    talk.politics.guns       0.95      0.93      0.94       364\n",
            " talk.politics.mideast       0.99      0.93      0.96       376\n",
            "\n",
            "              accuracy                           0.92      5477\n",
            "             macro avg       0.92      0.92      0.92      5477\n",
            "          weighted avg       0.92      0.92      0.92      5477\n",
            "\n"
          ]
        }
      ]
    }
  ]
}